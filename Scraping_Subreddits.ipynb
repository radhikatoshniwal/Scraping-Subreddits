{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping Subreddits",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALbTn_w4RL20"
      },
      "source": [
        "## Scrape the latest 2000 submissions from any subreddit. I've scraped r/SomebodyMakeThis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc5CDS29i04o"
      },
      "source": [
        "!pip install praw\n",
        "import praw\n",
        "reddit=praw.Reddit(client_id='yourClientID',client_secret='yourClientSecret',user_agent='Python bot v1.0');\n",
        "subreddit=\"SomebodyMakeThis\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE6wj4mFBi5u"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://api.pushshift.io/reddit/search/submission\"\n",
        "\n",
        "def crawl_page(subreddit: str, last_page = None):\n",
        "  \"\"\"Crawl a page of results from a given subreddit.\n",
        "\n",
        "  :param subreddit: The subreddit to crawl.\n",
        "  :param last_page: The last downloaded page.\n",
        "\n",
        "  :return: A page or results.\n",
        "  \"\"\"\n",
        "  params = {\"subreddit\": subreddit, \"size\": 500, \"sort\": \"desc\", \"sort_type\": \"created_utc\"}\n",
        "  if last_page is not None:\n",
        "    if len(last_page) > 0:\n",
        "      # resume from where we left at the last page\n",
        "      params[\"before\"] = last_page[-1][\"created_utc\"]\n",
        "    else:\n",
        "      # the last page was empty, we are past the last page\n",
        "      return []\n",
        "  results = requests.get(url, params)\n",
        "  if not results.ok:\n",
        "    # something wrong happened\n",
        "    raise Exception(\"Server returned status code {}\".format(results.status_code))\n",
        "  return results.json()[\"data\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJXjzG7jBp9m"
      },
      "source": [
        "import time\n",
        "\n",
        "def crawl_subreddit(subreddit, max_submissions = 2000):\n",
        "  \"\"\"\n",
        "  Crawl submissions from a subreddit.\n",
        "\n",
        "  :param subreddit: The subreddit to crawl.\n",
        "  :param max_submissions: The maximum number of submissions to download.\n",
        "\n",
        "  :return: A list of submissions.\n",
        "  \"\"\"\n",
        "  submissions = []\n",
        "  last_page = None\n",
        "  while last_page != [] and len(submissions) < max_submissions:\n",
        "    last_page = crawl_page(subreddit, last_page)\n",
        "    submissions += last_page\n",
        "    time.sleep(3)\n",
        "  return submissions[:max_submissions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12r3gQsMB2XR"
      },
      "source": [
        "lastest_submissions = crawl_subreddit(subreddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKX6H3uMGVMZ"
      },
      "source": [
        "## Getting author, idea, description, upvote_ratio from lastest_submissions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LErdSkkVD_Jf"
      },
      "source": [
        "title=[]\n",
        "author=[]\n",
        "description=[]\n",
        "url=[]\n",
        "for subs in lastest_submissions:\n",
        "  title.append(subs[\"title\"])\n",
        "  author.append(subs[\"author\"])\n",
        "  try:\n",
        "    description.append(subs[\"selftext\"])\n",
        "  except KeyError:\n",
        "    description.append(\"\")\n",
        "  url.append(subs[\"url\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n83xBbfNM7NP"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame()\n",
        "df[\"title\"]=title\n",
        "df[\"author\"]=author\n",
        "df[\"description\"]=description\n",
        "df[\"url\"]=url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzCYZBkRtL8z"
      },
      "source": [
        "df.index.rename('index',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSSXSqQwTITa"
      },
      "source": [
        "df.to_csv(\"submissions.csv\") # Storing the data as a csv file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65qVJMmR2u3"
      },
      "source": [
        "## Storing the data in Firebase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl7A7QqpqZvu"
      },
      "source": [
        "#get your key from firebase console\n",
        "import pandas as pd\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "cred = credentials.Certificate(\"yourKeyFileName.json\")\n",
        "db = firestore.client()\n",
        "tmp = df.to_dict(orient='records')\n",
        "for i in range(0,2000):\n",
        "  db.collection(u'submissions').document(i).set(tmp[i])\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}